{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb73297",
   "metadata": {},
   "source": [
    "## â†’ Chapter Five  \n",
    "*The Magic of Open APIs and Open Databases*\n",
    "\n",
    "ğŸ”ºAfter many failed attempts with Chinese websites, I decided to see how many titles I could find using just open databases based in the U.S., and planned to manually fill in the rest.  \n",
    "Even if only a fraction of the books were found, it would still save time.\n",
    "\n",
    "ğŸ”ºTo my surprise, I realized the managers had misunderstood the processand accidentally mislead me. When they said they couldnâ€™t retrieve Chinese book info by ISBN, what they really meant was that the data wouldnâ€™t auto-populate in Shopify through a barcode scanâ€”so they gave up and resorted to manual entry. It became clear that these resources had always been available, the people before me just didnâ€™t know where or how to look.  \n",
    "\n",
    "\n",
    "ğŸ”ºBut actually, there are several open database options that provide structured ISBN data and return a good range of results. Out of roughly 700 titles, **Zotero** (through WorldCat and other databases) returned ~200 results, **ISBNdb** returned ~520 results; and **isbnsearch.org** worked well for quick scraping but ran into a rate limit issue.\n",
    "\n",
    "Below, I provide the tutorial to:   \n",
    "- Use the **ISBNdb API** to retrieve detailed information on books by ISBNs\n",
    "- Scrape **isbnsearch.org** using BeautifulSoup and retrieve basic book information by ISBN without the book descriptions\n",
    "- Look up book info by ISBN in bulk through **Zotero**\n",
    "\n",
    "---\n",
    "## â†’ ç¬¬äº”ç«   \n",
    "*è¸ç ´é“é‹æ— è§…å¤„ï¼Œå¼€æ”¾æ•°æ®åœ¨çœ¼å‰*\n",
    "\n",
    "ğŸ”ºåœ¨å¤šæ¬¡å°è¯•ä¸­æ–‡ç½‘ç«™å¤±è´¥ä¹‹åï¼Œæˆ‘å†³å®šè¯•è¯•çœ‹ï¼Œå•çº¯ä¾èµ–ç¾å›½çš„å¼€æ”¾æ•°æ®åº“ï¼Œèƒ½æ‰¾åˆ°å¤šå°‘å›¾ä¹¦çš„ä¿¡æ¯ï¼Œå¹¶è®¡åˆ’æŠŠå‰©ä¸‹æ‰¾ä¸åˆ°çš„éƒ¨åˆ†æ‰‹åŠ¨è¡¥ä¸Šã€‚  \n",
    "å³ä½¿åªèƒ½æ‰¾å›ä¸€å°éƒ¨åˆ†ä¹¦ç›®ï¼Œä¹Ÿèƒ½èŠ‚çœä¸å°‘æ—¶é—´ã€‚\n",
    "\n",
    "ğŸ”ºä»¤æˆ‘æ„å¤–çš„æ˜¯ï¼Œæˆ‘æ„è¯†åˆ°åº—é‡Œçš„ç®¡ç†äººå‘˜å…¶å®è¯¯è§£äº†æ•´ä¸ªæµç¨‹ï¼Œå¹¶å› æ­¤è¯¯å¯¼äº†æˆ‘ã€‚ä»–ä»¬å£ä¸­çš„â€œæ— æ³•é€šè¿‡ISBNæŸ¥åˆ°ä¸­æ–‡ä¹¦ç±çš„ä¿¡æ¯â€ï¼Œå…¶å®æ˜¯æŒ‡è¿™äº›ä¿¡æ¯æ— æ³•é€šè¿‡æ‰«ç è‡ªåŠ¨å‡ºç°åœ¨Shopifyé‡Œã€‚äºæ˜¯ä»–ä»¬å°±æ”¾å¼ƒäº†è‡ªåŠ¨åŒ–çš„æ–¹æ³•ï¼Œè½¬è€Œå…¨é äººå·¥è¾“å…¥ã€‚åæ¥æˆ‘æ‰æ„è¯†åˆ°ï¼Œè¿™äº›èµ„æºå…¶å®ä¸€ç›´éƒ½åœ¨ï¼Œåªæ˜¯ä¹‹å‰æ²¡æœ‰äººçŸ¥é“è¯¥å»å“ªé‡Œæ‰¾ï¼Œæˆ–è€…æ€ä¹ˆç”¨ã€‚\n",
    "\n",
    "ğŸ”ºäº‹å®ä¸Šï¼Œå¾ˆå¤šå¼€æ”¾å¹³å°ç¡®å®æä¾›ç»“æ„åŒ–çš„ISBNä¹¦ç›®ä¿¡æ¯ï¼Œå¹¶èƒ½è¿”å›å¤§é‡ç»“æœã€‚åœ¨æˆ‘å¤§çº¦700æœ¬ä¹¦çš„æ ·æœ¬ä¸­ï¼š  \n",
    "- **Zotero**ï¼ˆé€šè¿‡ WorldCatï¼‰è¿”å›äº†çº¦ 200 æœ¬  \n",
    "- **ISBNdb** è¿”å›äº†çº¦ 520 æœ¬  \n",
    "- **isbnsearch.org** åˆ™é€‚åˆç”¨ BeautifulSoup å¿«é€Ÿçˆ¬å–åŸºç¡€ä¿¡æ¯ï¼Œä½†æœ‰è¯·æ±‚æ¬¡æ•°é™åˆ¶  \n",
    "\n",
    "ğŸ”ºæˆ‘é¦–å…ˆä½¿ç”¨äº† **ISBNdb API**ï¼Œå®ƒèƒ½ç¨³å®šè¿”å›ä¹¦ç±çš„å‡ºç‰ˆæ–¹ã€å‡ºç‰ˆæ—¥æœŸã€åˆ†ç±»æ ‡ç­¾ã€ç®€ä»‹ç­‰å­—æ®µã€‚  \n",
    "**Zotero** å…è®¸é€šè¿‡å®ƒçš„æ–‡çŒ®ç®¡ç†åŠŸèƒ½æ‰¹é‡æŸ¥è¯¢ISBNã€‚  \n",
    "**isbnsearch.org** åˆ™é€‚åˆå¿«é€Ÿã€è½»é‡åœ°çˆ¬å–åŸºç¡€å­—æ®µï¼ˆä¸å«ä¹¦ç±ç®€ä»‹ï¼‰ã€‚\n",
    "\n",
    "ä¸‹æ–¹æ˜¯å…³äºä»¥ä¸‹æ–¹æ³•çš„è¯¦ç»†æ•™ç¨‹ï¼š  \n",
    "- å¦‚ä½•ä½¿ç”¨ **ISBNdb API** é€šè¿‡ ISBN è·å–è¯¦ç»†å›¾ä¹¦ä¿¡æ¯  \n",
    "- å¦‚ä½•ä½¿ç”¨ **BeautifulSoup** çˆ¬å– **isbnsearch.org** å¹¶æå–åŸºç¡€å­—æ®µï¼ˆä¸å«ç®€ä»‹ï¼‰  \n",
    "- å¦‚ä½•é€šè¿‡ **Zotero** æ‰¹é‡æŸ¥è¯¢ ISBN å¹¶è·å–å›¾ä¹¦ä¿¡æ¯\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49bc8a0",
   "metadata": {},
   "source": [
    "### **â†’ I. Querying ISBNdb API for Book Information**  \n",
    "\n",
    "**â†’ 1.â—Requirements:**  \n",
    "ğŸ”ºYou must register on [ISBNdb.com](https://isbndb.com/) and purchase a plan to access the API.\n",
    "ğŸ”ºOnce registered, you will receive a personal API key, which you'll use for authorization.\n",
    "\n",
    "**â†’ 2. ISBNdb and API explained**  \n",
    "ğŸ”º[ISBNdb](https://isbndb.com/) is a subscription-based API that provides structured metadata for books.\n",
    "It supports queries by ISBN and is useful for bulk cataloging projects.\n",
    "\n",
    "ğŸ”ºAn **API** (Application Programming Interface) is a tool that allows different software systems to talk to each other.  \n",
    "In this case, we are sending a request from our Python script to **ISBNdb.com**, which is a website that stores metadata for books.  \n",
    "In response, the API sends us structured data about the book we ask for, based on its **ISBN** (International Standard Book Number).\n",
    "\n",
    "ğŸ”ºUsing the code below, I was able to:   \n",
    "- Send each isbn in my list to the ISBNdb API using the format `https://api2.isbndb.com/book/{isbn}`  \n",
    "- Saving the returned data into a structured csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# define range and api key\n",
    "start_idx = 234 # you can re-define the start and end ids to choose how many entries from the list you want to query at one time.\n",
    "end_idx = 335\n",
    "API_KEY = 'your_api_key_here'  # replace with your actual API key\n",
    "\n",
    "if API_KEY == '':\n",
    "    print('Error: you need to subscribe to ISBNdb.com and provide your API key.')\n",
    "\n",
    "# set headers\n",
    "HEADERS = {\n",
    "    'accept': 'application/json',\n",
    "    'Authorization': API_KEY,\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "# function to query by isbn\n",
    "def get_book_info(isbn):\n",
    "    url = f'https://api2.isbndb.com/book/{isbn}'\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"book\", {})\n",
    "        else:\n",
    "            print(f\"ISBN {isbn} not found or error ({response.status_code})\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching ISBN {isbn}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# process isbn list from csv\n",
    "def process_isbn_csv(csv_path, start_idx=0, end_idx=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'CODECONTENT' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'CODECONTENT' column with ISBNs.\")\n",
    "    \n",
    "    isbn_list = df['CODECONTENT'].astype(str).tolist()\n",
    "    if end_idx is None:\n",
    "        end_idx = len(isbn_list)\n",
    "\n",
    "    results = []\n",
    "    for idx, isbn in enumerate(isbn_list[start_idx:end_idx], start=start_idx):\n",
    "        isbn = isbn.strip()\n",
    "        print(f\"[{idx}] Fetching: {isbn}\")\n",
    "        book_data = get_book_info(isbn)\n",
    "        book_data['isbn_searched'] = isbn  \n",
    "        results.append(book_data)\n",
    "        time.sleep(1)  \n",
    "\n",
    "    result_df = pd.json_normalize(results)\n",
    "    return result_df\n",
    "\n",
    "# run script (make sure the csv file path, start id, end id are all correct)\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = process_isbn_csv('../data/scannedResults.csv', start_idx, end_idx)\n",
    "    print(result_df.head())\n",
    "    result_df.to_csv(\"isbndbResults.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d05331",
   "metadata": {},
   "source": [
    "### **â†’ II. Scraping isbnsearch.org for book data by ISBNs**\n",
    "\n",
    "**â†’ 1. Website Structure**  \n",
    "\n",
    "ğŸ”ºI used BeautifulSoup to scrape the isbnsearch.org website because it does not have any restrictions. I only introcuded a sleep time to be polite. \n",
    "\n",
    "ğŸ”ºMuch like of the process I used before with BeautifulSoup, I needed to find the right URL structure for the pages containing information for each book and extract the information from HTML. Scraping this site was even simpler because it has a simple URL structure that will lead me directly to the book info page given an ISBN. \n",
    "\n",
    "ğŸ”ºwith BS4 and request, I was able to perform the tasks listed below for the first 10 entries in my ISBN list:\n",
    "\n",
    "- Use `requests` and `BeautifulSoup` to fetch and parse HTML\n",
    "- Extract key fields from the `<div class=\"bookinfo\">` section\n",
    "- Save the results to a structured csv\n",
    "\n",
    "ğŸ”ºHowever, I encountered a reCAPTCHA after that. Even though the site's robots.txt allows all URLs on the site are allowed to be crawled, they still have a rate limiting in place. So I would recommend running the code as below with at least random sleep time and in small batches.\n",
    "\n",
    "  â†’ <img src=\"../images/0502.png\" alt=\"Add Items by Identifier in Zotero\" width=\"200px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb328eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "# path to CSV with scanned ISBNs\n",
    "csv_path = '../data/scannedResults.csv'\n",
    "\n",
    "# load ISBNs\n",
    "df = pd.read_csv(csv_path)\n",
    "isbn_list = df['CODECONTENT'][:100].astype(str).tolist()\n",
    "\n",
    "# base URL\n",
    "base_url = 'https://isbnsearch.org/isbn/'\n",
    "\n",
    "# headers to mimic real browser\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# store results\n",
    "results = []\n",
    "\n",
    "for idx, isbn in enumerate(isbn_list):\n",
    "    url = base_url + isbn\n",
    "    print(f\"[{idx}] Scraping: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch ISBN {isbn} â€” status {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        info_div = soup.find('div', class_='bookinfo')\n",
    "\n",
    "        if not info_div:\n",
    "            print(f\"No data found for ISBN {isbn}\")\n",
    "            continue\n",
    "\n",
    "        data = {\n",
    "            'isbn': isbn,\n",
    "            'title': info_div.find('h1').get_text(strip=True) if info_div.find('h1') else None,\n",
    "            'isbn_13': None,\n",
    "            'isbn_10': None,\n",
    "            'author': None,\n",
    "            'edition': None,\n",
    "            'binding': None,\n",
    "            'publisher': None,\n",
    "            'published': None,\n",
    "        }\n",
    "\n",
    "        # extract fields from <p> elements\n",
    "        for p in info_div.find_all('p'):\n",
    "            text = p.get_text(strip=True)\n",
    "            if text.startswith('ISBN-13:'):\n",
    "                data['isbn_13'] = p.text.replace('ISBN-13:', '').strip()\n",
    "            elif text.startswith('ISBN-10:'):\n",
    "                data['isbn_10'] = p.text.replace('ISBN-10:', '').strip()\n",
    "            elif text.startswith('Author:'):\n",
    "                data['author'] = p.text.replace('Author:', '').strip()\n",
    "            elif text.startswith('Edition:'):\n",
    "                data['edition'] = p.text.replace('Edition:', '').strip()\n",
    "            elif text.startswith('Binding:'):\n",
    "                data['binding'] = p.text.replace('Binding:', '').strip()\n",
    "            elif text.startswith('Publisher:'):\n",
    "                data['publisher'] = p.text.replace('Publisher:', '').strip()\n",
    "            elif text.startswith('Published:'):\n",
    "                data['published'] = p.text.replace('Published:', '').strip()\n",
    "\n",
    "        results.append(data)\n",
    "\n",
    "        # random sleep to reduce block risk\n",
    "        time.sleep(random.uniform(5, 10))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping ISBN {isbn}: {e}\")\n",
    "        continue\n",
    "\n",
    "# save to CSV\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv('../data/isbnsearchResults.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… Data saved to isbnsearchResults.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655d71c",
   "metadata": {},
   "source": [
    "### **â†’III. Bulk ISBN Lookup in Zotero**\n",
    "\n",
    "**â†’ 1. Downloading Zotero**\n",
    "\n",
    "[Zotero](https://www.zotero.org/) is a free and open-source reference management tool used by researchers to collect, organize, and cite sources. It also allows you to add books by ISBN, DOI, or other identifiers and automatically pulls metadata from a number of databases. For example, many of the books I retrieved via bulk ISBN import were matched through **WorldCat**.\n",
    "You can [download Zotero on the official website](https://www.zotero.org/download/). \n",
    "\n",
    "**â†’2. Combine ISBNs into a single string to query in Zotero**\n",
    "Zotero allows bulk look up of ISBNs, but I needed to put all the values in a single comma separated string in Python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/scannedResults.csv')\n",
    "isbn_string = \",\".join(df['CODECONTENT'].dropna().astype(str).tolist())\n",
    "print(isbn_string)\n",
    "```\n",
    "I then copied the string into the input box for \"Add Items by Identifier\" in Zotero, and wait for Zotero to process and load the results.\n",
    "\n",
    "  â†’ <img src=\"../images/0501.png\" alt=\"Add Items by Identifier in Zotero\" width=\"400px\">\n",
    "\n",
    "\n",
    "**â†’3. Export Library**\n",
    "\n",
    "Once all books are successfully looked up, I exported my library into a csv named **zoteroExport.csv** and saved it in the data folder.\n",
    "  â†’ <img src=\"../images/0503.png\" alt=\"Export Library in Zotero\" height=\"300px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91889be0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### **â†’ I. ä½¿ç”¨ ISBNdb API æŸ¥è¯¢å›¾ä¹¦ä¿¡æ¯**\n",
    "\n",
    "**â†’ 1.â—ä½¿ç”¨è¦æ±‚ï¼š**  \n",
    "ğŸ”ºä½ éœ€è¦åœ¨ [ISBNdb.com](https://isbndb.com/) æ³¨å†Œè´¦æˆ·ï¼Œå¹¶è´­ä¹°ä¸€ä¸ª API è®¢é˜…è®¡åˆ’ã€‚  \n",
    "ğŸ”ºæ³¨å†Œåï¼Œä½ ä¼šè·å¾—ä¸€ä¸ªä¸“å±çš„ API å¯†é’¥ï¼Œç”¨äºæˆæƒè®¿é—®æ¥å£ã€‚\n",
    "\n",
    "**â†’ 2. ä»€ä¹ˆæ˜¯ ISBNdb å’Œ APIï¼Ÿ**  \n",
    "ğŸ”º[ISBNdb](https://isbndb.com/) æ˜¯ä¸€ä¸ªåŸºäºè®¢é˜…çš„ API æœåŠ¡ï¼Œæä¾›ç»“æ„åŒ–çš„å›¾ä¹¦å…ƒæ•°æ®ã€‚  \n",
    "å®ƒæ”¯æŒé€šè¿‡ ISBN æŸ¥è¯¢å›¾ä¹¦ä¿¡æ¯ï¼Œé€‚åˆç”¨äºæ‰¹é‡å›¾ä¹¦ç›®å½•æ•´ç†ç­‰é¡¹ç›®ã€‚\n",
    "\n",
    "ğŸ”º**API**ï¼ˆåº”ç”¨ç¨‹åºæ¥å£ï¼‰æ˜¯ä¸€ç§è®©ä¸åŒè½¯ä»¶ç³»ç»Ÿä¹‹é—´è¿›è¡Œé€šä¿¡çš„å·¥å…·ã€‚  \n",
    "åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ Python è„šæœ¬å‘ **ISBNdb.com** å‘é€è¯·æ±‚ï¼Œå®ƒæ˜¯ä¸€ä¸ªå›¾ä¹¦å…ƒæ•°æ®çš„æ•°æ®åº“ç½‘ç«™ã€‚  \n",
    "API ä¼šæ ¹æ®æˆ‘ä»¬æä¾›çš„ **ISBNï¼ˆå›½é™…æ ‡å‡†ä¹¦å·ï¼‰**ï¼Œè¿”å›å¯¹åº”å›¾ä¹¦çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚\n",
    "\n",
    "ğŸ”ºé€šè¿‡ä¸‹é¢çš„ä»£ç ï¼Œæˆ‘å®ç°äº†ä»¥ä¸‹æ“ä½œï¼š  \n",
    "- å°†æˆ‘åˆ—è¡¨ä¸­çš„æ¯ä¸ª ISBN å‘é€è‡³ ISBNdb APIï¼Œä½¿ç”¨çš„åœ°å€æ ¼å¼ä¸º `https://api2.isbndb.com/book/{isbn}`  \n",
    "- å°†è¿”å›çš„æ•°æ®ä¿å­˜ä¸ºç»“æ„åŒ–çš„ CSV æ–‡ä»¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b38ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# define range and api key\n",
    "start_idx = 234 # you can re-define the start and end ids to choose how many entries from the list you want to query at one time.\n",
    "end_idx = 235\n",
    "API_KEY = 'your_api_key_here'  # replace with your actual API key\n",
    "\n",
    "if API_KEY == '':\n",
    "    print('Error: you need to subscribe to ISBNdb.com and provide your API key.')\n",
    "\n",
    "# set headers\n",
    "HEADERS = {\n",
    "    'accept': 'application/json',\n",
    "    'Authorization': API_KEY,\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "# function to query a single isbn\n",
    "def get_book_info(isbn):\n",
    "    url = f'https://api2.isbndb.com/book/{isbn}'\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"book\", {})\n",
    "        else:\n",
    "            print(f\"ISBN {isbn} not found or error ({response.status_code})\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching ISBN {isbn}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# process isbn list from csv\n",
    "def process_isbn_csv(csv_path, start_idx=0, end_idx=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'CODECONTENT' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'CODECONTENT' column with ISBNs.\")\n",
    "    \n",
    "    isbn_list = df['CODECONTENT'].astype(str).tolist()\n",
    "    if end_idx is None:\n",
    "        end_idx = len(isbn_list)\n",
    "\n",
    "    results = []\n",
    "    for idx, isbn in enumerate(isbn_list[start_idx:end_idx], start=start_idx):\n",
    "        isbn = isbn.strip()\n",
    "        print(f\"[{idx}] Fetching: {isbn}\")\n",
    "        book_data = get_book_info(isbn)\n",
    "        book_data['isbn_searched'] = isbn  \n",
    "        results.append(book_data)\n",
    "        time.sleep(1)  # to respect API rate limits\n",
    "\n",
    "    result_df = pd.json_normalize(results)\n",
    "    return result_df\n",
    "\n",
    "# run script (make sure the csv file path, start id, end id are all correct)\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = process_isbn_csv('../data/scannedResults.csv', start_idx, end_idx)\n",
    "    print(result_df.head())\n",
    "    result_df.to_csv(\"isbndbResults.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262a907",
   "metadata": {},
   "source": [
    "### **â†’ II. ä½¿ç”¨ isbnsearch.org é€šè¿‡ ISBN çˆ¬å–å›¾ä¹¦æ•°æ®**\n",
    "\n",
    "**â†’ 1. ç½‘ç«™ç»“æ„**\n",
    "\n",
    "ğŸ”ºæˆ‘ä½¿ç”¨ BeautifulSoup çˆ¬å–äº† isbnsearch.org ç½‘ç«™çš„æ•°æ®ï¼Œå› ä¸ºè¯¥ç½‘ç«™æ²¡æœ‰æ˜æ˜¾çš„åçˆ¬è™«æœºåˆ¶ã€‚æˆ‘åªæ·»åŠ äº†å»¶æ—¶ï¼ˆsleep timeï¼‰ä»¥ç¤ºç¤¼è²Œã€‚  \n",
    "\n",
    "ğŸ”ºå°±åƒæˆ‘ä¹‹å‰ä½¿ç”¨ BeautifulSoup çš„æµç¨‹ä¸€æ ·ï¼Œæˆ‘éœ€è¦æ‰¾åˆ°æ¯æœ¬ä¹¦ä¿¡æ¯é¡µé¢çš„æ­£ç¡® URL ç»“æ„ï¼Œå¹¶ä» HTML ä¸­æå–ä¿¡æ¯ã€‚  \n",
    "è¿™ä¸ªç½‘ç«™çš„ç»“æ„æ›´åŠ ç®€å•ï¼Œç›´æ¥é€šè¿‡ ISBN æ„å»º URL å³å¯è·³è½¬åˆ°å¯¹åº”çš„å›¾ä¹¦ä¿¡æ¯é¡µé¢ã€‚\n",
    "\n",
    "ğŸ”ºä½¿ç”¨ `requests` å’Œ `BeautifulSoup`ï¼Œæˆ‘å®Œæˆäº†ä»¥ä¸‹ä»»åŠ¡ï¼ˆä»¥æˆ‘çš„ ISBN åˆ—è¡¨ä¸­å‰ 10 ä¸ªæ¡ç›®ä¸ºä¾‹ï¼‰ï¼š\n",
    "\n",
    "- ä½¿ç”¨ `requests` è·å–ç½‘é¡µ HTML å†…å®¹ï¼Œå¹¶ç”¨ `BeautifulSoup` è§£æ  \n",
    "- ä» `<div class=\"bookinfo\">` åŒºå—ä¸­æå–å…³é”®å­—æ®µ  \n",
    "- å°†æå–çš„æ•°æ®ä¿å­˜ä¸ºç»“æ„åŒ–çš„ CSV æ–‡ä»¶  \n",
    "\n",
    "ğŸ”ºä¸è¿‡ï¼Œåœ¨è¿è¡Œäº†çº¦ 10 æ¬¡è¯·æ±‚ä¹‹åï¼Œæˆ‘é‡åˆ°äº† **reCAPTCHA éªŒè¯**ã€‚è™½ç„¶è¯¥ç½‘ç«™çš„ `robots.txt` æ–‡ä»¶å…è®¸çˆ¬å–æ‰€æœ‰è·¯å¾„ï¼Œä½†å®ƒä»¬ä»ç„¶è®¾ç½®äº†è®¿é—®é¢‘ç‡é™åˆ¶ã€‚\n",
    "\n",
    "â†’ <img src=\"../images/0502.png\" alt=\"Add Items by Identifier in Zotero\" width=\"200px\">\n",
    "\n",
    "\n",
    "å› æ­¤ï¼Œæˆ‘å»ºè®®åœ¨è¿è¡Œä¸‹é¢çš„ä»£ç æ—¶ï¼ŒåŠ¡å¿…æ·»åŠ  **éšæœºå»¶æ—¶**ï¼Œå¹¶å°†è¯·æ±‚æ‹†åˆ†æˆ **å°æ‰¹æ¬¡**è¿è¡Œï¼Œä»¥é¿å…è¢«å°é”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "# path to CSV with scanned ISBNs\n",
    "csv_path = '../data/scannedResults.csv'\n",
    "\n",
    "# load ISBNs\n",
    "df = pd.read_csv(csv_path)\n",
    "isbn_list = df['CODECONTENT'][:100].astype(str).tolist()\n",
    "\n",
    "# base URL\n",
    "base_url = 'https://isbnsearch.org/isbn/'\n",
    "\n",
    "# headers to mimic real browser\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# store results\n",
    "results = []\n",
    "\n",
    "for idx, isbn in enumerate(isbn_list):\n",
    "    url = base_url + isbn\n",
    "    print(f\"[{idx}] Scraping: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch ISBN {isbn} â€” status {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        info_div = soup.find('div', class_='bookinfo')\n",
    "\n",
    "        if not info_div:\n",
    "            print(f\"No data found for ISBN {isbn}\")\n",
    "            continue\n",
    "\n",
    "        data = {\n",
    "            'isbn': isbn,\n",
    "            'title': info_div.find('h1').get_text(strip=True) if info_div.find('h1') else None,\n",
    "            'isbn_13': None,\n",
    "            'isbn_10': None,\n",
    "            'author': None,\n",
    "            'edition': None,\n",
    "            'binding': None,\n",
    "            'publisher': None,\n",
    "            'published': None,\n",
    "        }\n",
    "\n",
    "        # extract fields from <p> elements\n",
    "        for p in info_div.find_all('p'):\n",
    "            text = p.get_text(strip=True)\n",
    "            if text.startswith('ISBN-13:'):\n",
    "                data['isbn_13'] = p.text.replace('ISBN-13:', '').strip()\n",
    "            elif text.startswith('ISBN-10:'):\n",
    "                data['isbn_10'] = p.text.replace('ISBN-10:', '').strip()\n",
    "            elif text.startswith('Author:'):\n",
    "                data['author'] = p.text.replace('Author:', '').strip()\n",
    "            elif text.startswith('Edition:'):\n",
    "                data['edition'] = p.text.replace('Edition:', '').strip()\n",
    "            elif text.startswith('Binding:'):\n",
    "                data['binding'] = p.text.replace('Binding:', '').strip()\n",
    "            elif text.startswith('Publisher:'):\n",
    "                data['publisher'] = p.text.replace('Publisher:', '').strip()\n",
    "            elif text.startswith('Published:'):\n",
    "                data['published'] = p.text.replace('Published:', '').strip()\n",
    "\n",
    "        results.append(data)\n",
    "\n",
    "        # random sleep to reduce block risk\n",
    "        time.sleep(random.uniform(5, 10))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping ISBN {isbn}: {e}\")\n",
    "        continue\n",
    "\n",
    "# save to CSV\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv('../data/isbnsearchResults.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… Data saved to isbnsearchResults.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7e855",
   "metadata": {},
   "source": [
    "### **â†’ III. ä½¿ç”¨ Zotero æ‰¹é‡æŸ¥è¯¢ ISBN å›¾ä¹¦ä¿¡æ¯**\n",
    "\n",
    "**â†’ 1. ä¸‹è½½ Zotero**\n",
    "\n",
    "[Zotero](https://www.zotero.org/) æ˜¯ä¸€ä¸ªå…è´¹å¼€æºçš„å‚è€ƒæ–‡çŒ®ç®¡ç†å·¥å…·ï¼Œè¢«å¹¿æ³›ç”¨äºæ”¶é›†ã€æ•´ç†å’Œå¼•ç”¨å„ç±»æ–‡çŒ®èµ„æ–™ã€‚å®ƒæ”¯æŒé€šè¿‡ ISBNã€DOI æˆ–å…¶ä»–è¯†åˆ«ç æ·»åŠ å›¾ä¹¦ï¼Œå¹¶å¯è‡ªåŠ¨ä»å¤šä¸ªæ•°æ®åº“ä¸­æŠ“å–å…ƒæ•°æ®ã€‚  \n",
    "ä¾‹å¦‚ï¼Œæˆ‘é€šè¿‡æ‰¹é‡å¯¼å…¥ ISBN æŸ¥è¯¢æ—¶ï¼ŒZotero å¾ˆå¤šç»“æœéƒ½æ¥è‡ªäº **WorldCat** æ•°æ®åº“ã€‚  \n",
    "ä½ å¯ä»¥åœ¨ [Zotero å®˜æ–¹ç½‘ç«™ä¸‹è½½](https://www.zotero.org/download/) æ¡Œé¢å®¢æˆ·ç«¯ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**â†’ 2. å°† ISBN åˆå¹¶ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ä»¥ä¾¿æŸ¥è¯¢**\n",
    "\n",
    "Zotero æ”¯æŒæ‰¹é‡ ISBN æŸ¥è¯¢ï¼Œä½†éœ€è¦å°†æ‰€æœ‰ ISBN åˆå¹¶ä¸ºä¸€ä¸ªé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ã€‚æˆ‘ä½¿ç”¨ä»¥ä¸‹ Python ä»£ç å®Œæˆè¿™ä¸€æ­¥ï¼š\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/scannedResults.csv')\n",
    "isbn_string = \",\".join(df['CODECONTENT'].dropna().astype(str).tolist())\n",
    "print(isbn_string)\n",
    "```\n",
    "å¤åˆ¶è¾“å‡ºçš„å­—ç¬¦ä¸²ï¼Œåœ¨ Zotero ä¸­ç‚¹å‡»ä¸Šæ–¹å·¥å…·æ çš„â€œAdd Items by Identifierâ€ï¼Œå°†å­—ç¬¦ä¸²ç²˜è´´è¿›è¾“å…¥æ¡†ï¼Œç„¶åæŒ‰ä¸‹ Enterï¼Œç­‰å¾… Zotero é€ä¸ªæŸ¥è¯¢å¹¶åŠ è½½å…ƒæ•°æ®ã€‚\n",
    "\n",
    "  â†’ <img src=\"../images/0501.png\" alt=\"Add Items by Identifier in Zotero\" width=\"400px\">\n",
    "\n",
    "**â†’ 3. å¯¼å‡ºå›¾ä¹¦åº“ä¸º CSV**\n",
    "\n",
    "å½“æ‰€æœ‰å›¾ä¹¦ä¿¡æ¯éƒ½æˆåŠŸæŸ¥æ‰¾åˆ°åï¼Œæˆ‘å°†å›¾ä¹¦åº“å¯¼å‡ºä¸ºåä¸º **zoteroExport.csv** çš„ CSV æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨ `data` æ–‡ä»¶å¤¹ä¸­ã€‚  \n",
    "  â†’ <img src=\"../images/0503.png\" alt=\"åœ¨ Zotero ä¸­å¯¼å‡ºå›¾ä¹¦åº“\" height=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503c35c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
