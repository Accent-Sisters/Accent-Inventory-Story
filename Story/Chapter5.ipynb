{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb73297",
   "metadata": {},
   "source": [
    "## → Chapter Five  \n",
    "*The Magic of Open APIs and ISBN Lookups*\n",
    "\n",
    "🔺After many failed attempts with Chinese websites, I decided to see how many titles I could find using just open databases based in the U.S., and planned to manually fill in the rest.  \n",
    "Even if only a fraction of the books were found, it would still save time.\n",
    "\n",
    "🔺To my surprise, I realized the managers had misunderstood the processand accidentally mislead me. When they said they couldn’t retrieve Chinese book info by ISBN, what they really meant was that the data wouldn’t auto-populate in Shopify through a barcode scan—so they gave up and resorted to manual entry. It became clear that these resources had always been available, the people before me just didn’t know where or how to look.  \n",
    "\n",
    "\n",
    "🔺But in reality, many open platforms do provide structured ISBN data and return a wide range of results. Out of roughly 700 titles:  \n",
    "- **Zotero** (via WorldCat) returned ~200  \n",
    "- **ISBNdb** returned ~520  \n",
    "- **isbnsearch.org** worked well for quick scraping using BeautifulSoup  \n",
    "\n",
    "🔺I first used the **ISBNdb API**, which returned reliable metadata including publisher, date, genre, and description.  \n",
    "**Zotero** provided bulk query options through its reference management interface.  \n",
    "**isbnsearch.org** allowed lightweight scraping for additional results.\n",
    "\n",
    "Below, I provide the tutorial to:   \n",
    "- Use the **ISBNdb API** to retrieve detailed information on books by ISBNs\n",
    "- Scrape **isbnsearch.org** using BeautifulSoup and retrieve basic book information by ISBN without the book descriptions\n",
    "- Look up book info by ISBN in bulk through **Zotero**\n",
    "\n",
    "---\n",
    "## → 第五章  \n",
    "*踏破铁鞋无觅处，开放数据在身边*\n",
    "\n",
    "🔺在多次尝试中文网站失败之后，我决定试试看，单纯依赖美国的开放数据库，能找到多少图书的信息，并计划把剩下找不到的部分手动补上。  \n",
    "即使只能找回一小部分书目，也能节省不少时间。\n",
    "\n",
    "🔺令我意外的是，我意识到店里的管理人员其实误解了整个流程，并因此误导了我。他们口中的“无法通过ISBN查到中文书籍的信息”，其实是指这些信息无法通过扫码自动出现在Shopify里。于是他们就放弃了自动化的方法，转而全靠人工输入。后来我才意识到，这些资源其实一直都在，只是之前没有人知道该去哪里找，或者怎么用。\n",
    "\n",
    "🔺事实上，很多开放平台确实提供结构化的ISBN书目信息，并能返回大量结果。在我大约700本书的样本中：  \n",
    "- **Zotero**（通过 WorldCat）返回了约 200 本  \n",
    "- **ISBNdb** 返回了约 520 本  \n",
    "- **isbnsearch.org** 则适合用 BeautifulSoup 快速爬取基础信息  \n",
    "\n",
    "🔺我首先使用了 **ISBNdb API**，它能稳定返回书籍的出版方、出版日期、分类标签、简介等字段。  \n",
    "**Zotero** 允许通过它的文献管理功能批量查询ISBN。  \n",
    "**isbnsearch.org** 则适合快速、轻量地爬取基础字段（不含书籍简介）。\n",
    "\n",
    "下方是关于以下方法的详细教程：  \n",
    "- 如何使用 **ISBNdb API** 通过 ISBN 获取详细图书信息  \n",
    "- 如何使用 **BeautifulSoup** 爬取 **isbnsearch.org** 并提取基础字段（不含简介）  \n",
    "- 如何通过 **Zotero** 批量查询 ISBN 并获取图书信息\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49bc8a0",
   "metadata": {},
   "source": [
    "### **→ I. Querying ISBNdb API for Book Information**  \n",
    "\n",
    "**→ 1.❗Requirements:**  \n",
    "🔺You must register on [ISBNdb.com](https://isbndb.com/) and purchase a plan to access the API.\n",
    "🔺Once registered, you will receive a personal API key, which you'll use for authorization.\n",
    "\n",
    "**→ 2. ISBNdb and API explained**  \n",
    "🔺[ISBNdb](https://isbndb.com/) is a subscription-based API that provides structured metadata for books.\n",
    "It supports queries by ISBN and is useful for bulk cataloging projects.\n",
    "\n",
    "🔺An **API** (Application Programming Interface) is a tool that allows different software systems to talk to each other.  \n",
    "In this case, we are sending a request from our Python script to **ISBNdb.com**, which is a website that stores metadata for books.  \n",
    "In response, the API sends us structured data about the book we ask for, based on its **ISBN** (International Standard Book Number).\n",
    "\n",
    "🔺Using the code below, I was able to:   \n",
    "- Send each isbn in my list to the ISBNdb API using the format `https://api2.isbndb.com/book/{isbn}`  \n",
    "- Saving the returned data into a structured csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a4262",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# define range and api key\n",
    "start_idx = 234 # you can re-define the start and end ids to choose how many entries from the list you want to query at one time.\n",
    "end_idx = 235\n",
    "API_KEY = 'your_api_key_here'  # replace with your actual API key\n",
    "\n",
    "if API_KEY == '':\n",
    "    print('Error: you need to subscribe to ISBNdb.com and provide your API key.')\n",
    "\n",
    "# set headers\n",
    "HEADERS = {\n",
    "    'accept': 'application/json',\n",
    "    'Authorization': API_KEY,\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "# function to query a single isbn\n",
    "def get_book_info(isbn):\n",
    "    url = f'https://api2.isbndb.com/book/{isbn}'\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"book\", {})\n",
    "        else:\n",
    "            print(f\"ISBN {isbn} not found or error ({response.status_code})\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching ISBN {isbn}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# process isbn list from csv\n",
    "def process_isbn_csv(csv_path, start_idx=0, end_idx=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'CODECONTENT' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'CODECONTENT' column with ISBNs.\")\n",
    "    \n",
    "    isbn_list = df['CODECONTENT'].astype(str).tolist()\n",
    "    if end_idx is None:\n",
    "        end_idx = len(isbn_list)\n",
    "\n",
    "    results = []\n",
    "    for idx, isbn in enumerate(isbn_list[start_idx:end_idx], start=start_idx):\n",
    "        isbn = isbn.strip()\n",
    "        print(f\"[{idx}] Fetching: {isbn}\")\n",
    "        book_data = get_book_info(isbn)\n",
    "        book_data['isbn_searched'] = isbn  \n",
    "        results.append(book_data)\n",
    "        time.sleep(1)  # to respect API rate limits\n",
    "\n",
    "    result_df = pd.json_normalize(results)\n",
    "    return result_df\n",
    "\n",
    "# run script (make sure the csv file path, start id, end id are all correct)\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = process_isbn_csv('../data/scannedResults.csv', start_idx, end_idx)\n",
    "    print(result_df.head())\n",
    "    result_df.to_csv(\"isbndbResults.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d05331",
   "metadata": {},
   "source": [
    "### **→ II. Scraping isbnsearch.org for book data by ISBNs**\n",
    "\n",
    "**→ 1. Website Structure**  \n",
    "\n",
    "🔺I used BeautifulSoup to scrape the isbnsearch.org website because it does not have any restrictions. I only introcuded a sleep time to be polite. \n",
    "\n",
    "🔺Much like of the process I used before with BeautifulSoup, I needed to find the right URL structure for the pages containing information for each book and extract the information from HTML. Scraping this site was even simpler because it has a simple URL structure that will lead me directly to the book info page given an ISBN. \n",
    "\n",
    "🔺With the code below, I was able to: \n",
    "\n",
    "- Generate a URL for each ISBN using the pattern `https://isbnsearch.org/isbn/{isbn}`\n",
    "- Use `requests` and `BeautifulSoup` to fetch and parse HTML\n",
    "- Extract key fields from the `<div class=\"bookinfo\">` section\n",
    "- Save the results to a structured csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb328eee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# path to csv with scanned ISBNs\n",
    "csv_path = '../data/scannedResults.csv'\n",
    "\n",
    "# load ISBNs\n",
    "df = pd.read_csv(csv_path)\n",
    "isbn_list = df['CODECONTENT'].astype(str).tolist()\n",
    "\n",
    "# base URL for isbnsearch\n",
    "base_url = 'https://isbnsearch.org/isbn/'\n",
    "\n",
    "# list to store results\n",
    "results = []\n",
    "\n",
    "for idx, isbn in enumerate(isbn_list):\n",
    "    url = base_url + isbn\n",
    "    print(f\"[{idx}] Scraping: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch ISBN {isbn} — status {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        info_div = soup.find('div', class_='bookinfo')\n",
    "\n",
    "        if not info_div:\n",
    "            print(f\"No data found for ISBN {isbn}\")\n",
    "            continue\n",
    "\n",
    "        data = {\n",
    "            'isbn': isbn,\n",
    "            'title': info_div.find('h1').get_text(strip=True) if info_div.find('h1') else None,\n",
    "            'isbn_13': None,\n",
    "            'isbn_10': None,\n",
    "            'author': None,\n",
    "            'edition': None,\n",
    "            'binding': None,\n",
    "            'publisher': None,\n",
    "            'published': None,\n",
    "        }\n",
    "\n",
    "        # parse each <p> field\n",
    "        for p in info_div.find_all('p'):\n",
    "            text = p.get_text(strip=True)\n",
    "            if text.startswith('ISBN-13:'):\n",
    "                data['isbn_13'] = p.text.replace('ISBN-13:', '').strip()\n",
    "            elif text.startswith('ISBN-10:'):\n",
    "                data['isbn_10'] = p.text.replace('ISBN-10:', '').strip()\n",
    "            elif text.startswith('Author:'):\n",
    "                data['author'] = p.text.replace('Author:', '').strip()\n",
    "            elif text.startswith('Edition:'):\n",
    "                data['edition'] = p.text.replace('Edition:', '').strip()\n",
    "            elif text.startswith('Binding:'):\n",
    "                data['binding'] = p.text.replace('Binding:', '').strip()\n",
    "            elif text.startswith('Publisher:'):\n",
    "                data['publisher'] = p.text.replace('Publisher:', '').strip()\n",
    "            elif text.startswith('Published:'):\n",
    "                data['published'] = p.text.replace('Published:', '').strip()\n",
    "\n",
    "        results.append(data)\n",
    "        time.sleep(1)  # use a sleep time to be polite (avoid too many requests at once)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping ISBN {isbn}: {e}\")\n",
    "        continue\n",
    "\n",
    "# convert to DataFrame and save\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv('../data/isbnsearch_results.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Data saved to isbnsearch_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655d71c",
   "metadata": {},
   "source": [
    "### **→III. Bulk ISBN Lookup in Zotero**\n",
    "\n",
    "**→ 1. Downloading Zotero**\n",
    "\n",
    "[Zotero](https://www.zotero.org/) is a free and open-source reference management tool used by researchers to collect, organize, and cite sources. It also allows you to add books by ISBN, DOI, or other identifiers and automatically pulls metadata from a number of databases. For example, many of the books I retrieved via bulk ISBN import were matched through **WorldCat**.\n",
    "You can [download Zotero on the official website](https://www.zotero.org/download/). \n",
    "\n",
    "**→2. Combine ISBNs into a single string to query in Zotero**\n",
    "Zotero allows bulk look up of ISBNs, but I needed to put all the values in a single comma separated string in Python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/scannedResults.csv')\n",
    "isbn_string = \",\".join(df['CODECONTENT'].dropna().astype(str).tolist())\n",
    "print(isbn_string)\n",
    "```\n",
    "I then copied the string into the input box for \"Add Items by Identifier\" in Zotero, and wait for Zotero to process and load the results.\n",
    "\n",
    "\n",
    "**4. Export Library**\n",
    "\n",
    "Once all books are successfully looked up, I exported my library into a csv named **zoteroExport.csv** and saved it in the data folder.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
