{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a84307",
   "metadata": {},
   "source": [
    "## → Chapter Four  \n",
    "*Getting Lost in Dangdang.com’s Chaotic Marketplace*\n",
    "\n",
    "🔺After bookschina, I tried scraping [Dangdang.com](https://www.dangdang.com) using a similar Selenium setup. The method was the same:  \n",
    "search by ISBN → scrape the search result page → extract product URLs → scrape product detail pages.  \n",
    "\n",
    "---\n",
    "\n",
    "## → 第四章  \n",
    "*乱入当当书市之险途*\n",
    "\n",
    "🔺在成功爬取 bookschina 之后，我尝试使用类似的 Selenium 设置来爬取 [Dangdang.com](https://www.dangdang.com)。方法相同：  \n",
    "通过 ISBN 进行搜索 → 爬取搜索结果页 → 提取商品链接 → 爬取商品详情页。  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ee403",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **→ Use Selenium to Scrape Book Titles from Dangdang**\n",
    "\n",
    "Combining a similar website structure logic from the previous web scraping code for bookschina.com and the method using Selenium, I used the following code to: \n",
    "\n",
    "- Simulate real browswer interaction to search for books by their ISBNs on dangdang.com\n",
    "- Extract titles and links from the search result page  \n",
    "- Save the result to a CSV file\n",
    "\n",
    "---\n",
    "\n",
    "### **→ 使用 Selenium 从当当网爬取图书标题**\n",
    "\n",
    "结合前面在 bookschina.com 的网页结构分析方法，以及使用 Selenium 模拟浏览器操作的思路，我使用了以下代码来完成以下任务：\n",
    "\n",
    "- 模拟真实浏览器行为，在当当网根据 ISBN 搜索图书  \n",
    "- 从搜索结果页面中提取图书标题和链接  \n",
    "- 将结果保存为 CSV 文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8067e1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# set path to chromedriver\n",
    "driver_path = \"C:/Users/aliso/Downloads/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "# configure browser\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/135.0.0.0 Safari/537.36\")\n",
    "\n",
    "service = Service(executable_path=driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# load ISBNs from CSV\n",
    "df = pd.read_csv('../data/scannedResults.csv', encoding=\"utf-8\")\n",
    "all_books = []\n",
    "\n",
    "# loop through a range of ISBNs (you can modify the range to specify how many entries you want to query at one time)\n",
    "for idx in range(0, 250):\n",
    "    isbn = str(df.loc[idx, 'CODECONTENT']).strip()\n",
    "    if not isbn or isbn == 'nan':\n",
    "        print(f\"skipping index {idx} — invalid isbn\")\n",
    "        continue\n",
    "\n",
    "    print(f\"searching isbn: {isbn}\")\n",
    "    search_url = f\"https://search.dangdang.com/?medium=01&key4={isbn}&category_path=01.00.00.00.00.00\"\n",
    "    driver.get(search_url)\n",
    "\n",
    "    # random delay to mimic human behavior\n",
    "    sleep_time = random.uniform(3, 8)\n",
    "    print(f\"sleeping for {sleep_time:.2f} seconds...\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    # try to find product listing blocks\n",
    "    containers = [\n",
    "        \"//div[@id='search_nature_rg']\",\n",
    "        \"//div[@class='con shoplist']\",\n",
    "        \"//div[@id='search_component_1']\"\n",
    "    ]\n",
    "\n",
    "    li_elements = []\n",
    "    for container_xpath in containers:\n",
    "        try:\n",
    "            container = driver.find_element(By.XPATH, container_xpath)\n",
    "            li_elements = container.find_elements(By.XPATH, \".//li[@ddt-pit='1']\")\n",
    "            if li_elements:\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            continue\n",
    "\n",
    "    if not li_elements:\n",
    "        print(\"no book entries found.\")\n",
    "        continue\n",
    "\n",
    "    # extract title and url from each result\n",
    "    for li in li_elements:\n",
    "        try:\n",
    "            try:\n",
    "                a_tag = li.find_element(By.XPATH, \".//p[@class='name']/a\")\n",
    "            except:\n",
    "                try:\n",
    "                    a_tag = li.find_element(By.CSS_SELECTOR, \"p.name a\")\n",
    "                except:\n",
    "                    a_tag = li.find_element(By.CSS_SELECTOR, \"a[name='itemlist-title']\")\n",
    "            \n",
    "            title = a_tag.get_attribute(\"title\").strip()\n",
    "            url = a_tag.get_attribute(\"href\")\n",
    "            print(f\"found book: {title}\")\n",
    "\n",
    "            all_books.append({\n",
    "                \"isbn\": isbn,\n",
    "                \"title\": title,\n",
    "                \"url\": url\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error extracting title: {e}\")\n",
    "            continue\n",
    "\n",
    "# save results to csv\n",
    "result_df = pd.DataFrame(all_books)\n",
    "if not result_df.empty:\n",
    "    result_df.to_csv(\"dangdangResults.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(\"saved to dangdangResults.csv\")\n",
    "\n",
    "# close browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5e3a3",
   "metadata": {},
   "source": [
    "🔺At first, this worked. I was able to extract basic metadata like title, publisher, author, and publication date from the **search result pages**.  \n",
    "However, Dangdang implements login restrictions and rate-limiting on the **product detail pages**, which stopped further scraping after a few queries.  \n",
    "\n",
    "→ <img src=\"../images/0402.png\" alt=\"dangdanglogin page\" width=\"300px\">  \n",
    "\n",
    "🔺Another issue: the \"titles\" in the search results were filled with SEO junk—there’s no clean or official book title field.  \n",
    "This created messy, inconsistent metadata for use in inventory management. (only the highlighted texts are the actual book titles)\n",
    "\n",
    "→ <img src=\"../images/0403.png\" alt=\"dangdang title examples\" width=\"300px\">  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "🔺一开始进展顺利。我能够从**搜索结果页**中提取出基本的元数据，比如书名、出版社、作者和出版日期。  \n",
    "但当我尝试访问**商品详情页**来获取更多信息时，Dangdang 启用了登录限制和访问频率限制，几次请求后就被阻止继续爬取。\n",
    "\n",
    "→ <img src=\"../images/0402.png\" alt=\"dangdanglogin page\" width=\"300px\">  \n",
    "\n",
    "🔺还有一个问题是：搜索结果中的“书名”充满了 SEO 垃圾词汇——没有一个干净、标准的字段来表示真实的书名。  \n",
    "这导致爬取下来的元数据非常混乱，不适合直接用于库存管理。\n",
    "\n",
    "→ <img src=\"../images/0403.png\" alt=\"dangdang title examples\" width=\"300px\">  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
