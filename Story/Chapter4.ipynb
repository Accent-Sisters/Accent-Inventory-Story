{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a84307",
   "metadata": {},
   "source": [
    "## â†’ Chapter Four  \n",
    "*Getting Lost in Dangdang.comâ€™s Chaotic Marketplace*\n",
    "\n",
    "ğŸ”ºAfter bookschina, I tried scraping [Dangdang.com](https://www.dangdang.com) using a similar Selenium setup. The method was the same:  \n",
    "search by ISBN â†’ scrape the search result page â†’ extract product URLs â†’ scrape product detail pages.  \n",
    "\n",
    "---\n",
    "\n",
    "## â†’ ç¬¬å››ç«   \n",
    "*ä¹±å…¥å½“å½“ä¹¦å¸‚ä¹‹é™©é€”*\n",
    "\n",
    "ğŸ”ºåœ¨æˆåŠŸçˆ¬å– bookschina ä¹‹åï¼Œæˆ‘å°è¯•ä½¿ç”¨ç±»ä¼¼çš„ Selenium è®¾ç½®æ¥çˆ¬å– [Dangdang.com](https://www.dangdang.com)ã€‚æ–¹æ³•ç›¸åŒï¼š  \n",
    "é€šè¿‡ ISBN è¿›è¡Œæœç´¢ â†’ çˆ¬å–æœç´¢ç»“æœé¡µ â†’ æå–å•†å“é“¾æ¥ â†’ çˆ¬å–å•†å“è¯¦æƒ…é¡µã€‚  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ee403",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **â†’ Use Selenium to Scrape Book Titles from Dangdang**\n",
    "\n",
    "Combining a similar website structure logic from the previous web scraping code for bookschina.com and the method using Selenium, I used the following code to: \n",
    "\n",
    "- Simulate real browswer interaction to search for books by their ISBNs on dangdang.com\n",
    "- Extract titles and links from the search result page  \n",
    "- Save the result to a CSV file\n",
    "\n",
    "---\n",
    "\n",
    "### **â†’ ä½¿ç”¨ Selenium ä»å½“å½“ç½‘çˆ¬å–å›¾ä¹¦æ ‡é¢˜**\n",
    "\n",
    "ç»“åˆå‰é¢åœ¨ bookschina.com çš„ç½‘é¡µç»“æ„åˆ†ææ–¹æ³•ï¼Œä»¥åŠä½¿ç”¨ Selenium æ¨¡æ‹Ÿæµè§ˆå™¨æ“ä½œçš„æ€è·¯ï¼Œæˆ‘ä½¿ç”¨äº†ä»¥ä¸‹ä»£ç æ¥å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n",
    "\n",
    "- æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸ºï¼Œåœ¨å½“å½“ç½‘æ ¹æ® ISBN æœç´¢å›¾ä¹¦  \n",
    "- ä»æœç´¢ç»“æœé¡µé¢ä¸­æå–å›¾ä¹¦æ ‡é¢˜å’Œé“¾æ¥  \n",
    "- å°†ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8067e1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# set path to chromedriver\n",
    "driver_path = \"C:/Users/aliso/Downloads/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "# configure browser\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/135.0.0.0 Safari/537.36\")\n",
    "\n",
    "service = Service(executable_path=driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# load ISBNs from CSV\n",
    "df = pd.read_csv('../data/scannedResults.csv', encoding=\"utf-8\")\n",
    "all_books = []\n",
    "\n",
    "# loop through a range of ISBNs (you can modify the range to specify how many entries you want to query at one time)\n",
    "for idx in range(0, 250):\n",
    "    isbn = str(df.loc[idx, 'CODECONTENT']).strip()\n",
    "    if not isbn or isbn == 'nan':\n",
    "        print(f\"skipping index {idx} â€” invalid isbn\")\n",
    "        continue\n",
    "\n",
    "    print(f\"searching isbn: {isbn}\")\n",
    "    search_url = f\"https://search.dangdang.com/?medium=01&key4={isbn}&category_path=01.00.00.00.00.00\"\n",
    "    driver.get(search_url)\n",
    "\n",
    "    # random delay to mimic human behavior\n",
    "    sleep_time = random.uniform(3, 8)\n",
    "    print(f\"sleeping for {sleep_time:.2f} seconds...\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    # try to find product listing blocks\n",
    "    containers = [\n",
    "        \"//div[@id='search_nature_rg']\",\n",
    "        \"//div[@class='con shoplist']\",\n",
    "        \"//div[@id='search_component_1']\"\n",
    "    ]\n",
    "\n",
    "    li_elements = []\n",
    "    for container_xpath in containers:\n",
    "        try:\n",
    "            container = driver.find_element(By.XPATH, container_xpath)\n",
    "            li_elements = container.find_elements(By.XPATH, \".//li[@ddt-pit='1']\")\n",
    "            if li_elements:\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            continue\n",
    "\n",
    "    if not li_elements:\n",
    "        print(\"no book entries found.\")\n",
    "        continue\n",
    "\n",
    "    # extract title and url from each result\n",
    "    for li in li_elements:\n",
    "        try:\n",
    "            try:\n",
    "                a_tag = li.find_element(By.XPATH, \".//p[@class='name']/a\")\n",
    "            except:\n",
    "                try:\n",
    "                    a_tag = li.find_element(By.CSS_SELECTOR, \"p.name a\")\n",
    "                except:\n",
    "                    a_tag = li.find_element(By.CSS_SELECTOR, \"a[name='itemlist-title']\")\n",
    "            \n",
    "            title = a_tag.get_attribute(\"title\").strip()\n",
    "            url = a_tag.get_attribute(\"href\")\n",
    "            print(f\"found book: {title}\")\n",
    "\n",
    "            all_books.append({\n",
    "                \"isbn\": isbn,\n",
    "                \"title\": title,\n",
    "                \"url\": url\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error extracting title: {e}\")\n",
    "            continue\n",
    "\n",
    "# save results to csv\n",
    "result_df = pd.DataFrame(all_books)\n",
    "if not result_df.empty:\n",
    "    result_df.to_csv(\"dangdangResults.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(\"saved to dangdangResults.csv\")\n",
    "\n",
    "# close browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5e3a3",
   "metadata": {},
   "source": [
    "ğŸ”ºAt first, this worked. I was able to extract basic metadata like title, publisher, author, and publication date from the **search result pages**.  \n",
    "However, Dangdang implements login restrictions and rate-limiting on the **product detail pages**, which stopped further scraping after a few queries.  \n",
    "\n",
    "â†’ <img src=\"../images/0402.png\" alt=\"dangdanglogin page\" width=\"300px\">  \n",
    "\n",
    "ğŸ”ºAnother issue: the \"titles\" in the search results were filled with SEO junkâ€”thereâ€™s no clean or official book title field.  \n",
    "This created messy, inconsistent metadata for use in inventory management. (only the highlighted texts are the actual book titles)\n",
    "\n",
    "â†’ <img src=\"../images/0403.png\" alt=\"dangdang title examples\" width=\"300px\">  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ”ºä¸€å¼€å§‹è¿›å±•é¡ºåˆ©ã€‚æˆ‘èƒ½å¤Ÿä»**æœç´¢ç»“æœé¡µ**ä¸­æå–å‡ºåŸºæœ¬çš„å…ƒæ•°æ®ï¼Œæ¯”å¦‚ä¹¦åã€å‡ºç‰ˆç¤¾ã€ä½œè€…å’Œå‡ºç‰ˆæ—¥æœŸã€‚  \n",
    "ä½†å½“æˆ‘å°è¯•è®¿é—®**å•†å“è¯¦æƒ…é¡µ**æ¥è·å–æ›´å¤šä¿¡æ¯æ—¶ï¼ŒDangdang å¯ç”¨äº†ç™»å½•é™åˆ¶å’Œè®¿é—®é¢‘ç‡é™åˆ¶ï¼Œå‡ æ¬¡è¯·æ±‚åå°±è¢«é˜»æ­¢ç»§ç»­çˆ¬å–ã€‚\n",
    "\n",
    "â†’ <img src=\"../images/0402.png\" alt=\"dangdanglogin page\" width=\"300px\">  \n",
    "\n",
    "ğŸ”ºè¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯ï¼šæœç´¢ç»“æœä¸­çš„â€œä¹¦åâ€å……æ»¡äº† SEO åƒåœ¾è¯æ±‡â€”â€”æ²¡æœ‰ä¸€ä¸ªå¹²å‡€ã€æ ‡å‡†çš„å­—æ®µæ¥è¡¨ç¤ºçœŸå®çš„ä¹¦åã€‚  \n",
    "è¿™å¯¼è‡´çˆ¬å–ä¸‹æ¥çš„å…ƒæ•°æ®éå¸¸æ··ä¹±ï¼Œä¸é€‚åˆç›´æ¥ç”¨äºåº“å­˜ç®¡ç†ã€‚\n",
    "\n",
    "â†’ <img src=\"../images/0403.png\" alt=\"dangdang title examples\" width=\"300px\">  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
