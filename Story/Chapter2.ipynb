{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6193bac",
   "metadata": {},
   "source": [
    "## â†’ Chapter Two  \n",
    "*Dreaming up Automation, Wielding the Magic Scanner* \n",
    "\n",
    "ğŸ”ºFaced with the frustration of manual data entry, I hoped that ISBN scanning could automate and speed up the process. My idea was to take one day to scan all the books before they got moved around again, then use the ISBNs to query book information from a database, and bulk import everything into Shopify. That sounded like a monthâ€™s worth of work condensed into two days.  \n",
    "\n",
    "ğŸ”ºInitially, I was told the store had already tried using ISBNs to automatically retrieve product information on Shopify, but failedâ€”most of the books were in Chinese and not supported by U.S.-based databases. Still, I believed there must be a way to automate this using web scraping. I started researching Chinese book databases or websites that might host the book metadata I needed.  \n",
    "\n",
    "ğŸ”ºI found a Chinese online book retailer called [bookschina.com](https://bookschina.com/) and ran a test scrape using around 70 ISBNs. Using a Python library called BeautifulSoup, I was able to successfully extract book information. Below is a short example of how I used Python and BeautifulSoup to scrape data from the retailerâ€™s website.\n",
    "\n",
    "---\n",
    "## â†’ ç¬¬äºŒå›\n",
    "*å°æ–‡å‘˜æ¢¦ä¼šè‡ªåŠ¨æœ¯ æ‰«æä»ªåœ¨æ‰‹å¦‚æœ‰ç¥åŠ©*\n",
    "\n",
    "ğŸ”ºé¢å¯¹æ‰‹åŠ¨å½•å…¥æ•°æ®çš„æŒ«è´¥æ„Ÿï¼Œæˆ‘å¼€å§‹å¸Œæœ›èƒ½é€šè¿‡ISBNæ‰«ææ¥å®ç°è‡ªåŠ¨åŒ–ï¼ŒåŠ å¿«è¿›åº¦ã€‚æˆ‘è®¾æƒ³åªè¦èŠ±ä¸€å¤©æ—¶é—´ï¼ŒæŠŠæ‰€æœ‰ä¹¦ç±çš„ISBNæ‰«å®Œï¼Œåœ¨å®ƒä»¬å†æ¬¡è¢«ç§»æ¥ç§»å»ä¹‹å‰ä¿å­˜ä¸‹æ¥ï¼Œç„¶åé€šè¿‡æŸä¸ªæ•°æ®åº“ç”¨ISBNæ‰¹é‡æŸ¥è¯¢ä¹¦ç±ä¿¡æ¯ï¼Œæœ€åç»Ÿä¸€å¯¼å…¥Shopifyã€‚è¿™å¬èµ·æ¥åƒæ˜¯èƒ½æŠŠä¸€ä¸ªæœˆçš„å·¥ä½œå‹ç¼©æˆä¸¤å¤©å®Œæˆçš„ç†æƒ³æ–¹æ¡ˆã€‚\n",
    "\n",
    "ğŸ”ºä¸è¿‡ä¸€å¼€å§‹æˆ‘è¢«å‘ŠçŸ¥ï¼Œåº—é‡Œä»¥å‰è¯•è¿‡ç”¨ISBNæ¥è‡ªåŠ¨å¯¼å…¥Shopifyäº§å“ä¿¡æ¯ï¼Œä½†æ²¡æˆåŠŸâ€”â€”å› ä¸ºå¤§å¤šæ•°ä¹¦æ˜¯ä¸­æ–‡çš„ï¼Œç”¨ç¾å›½çš„æœåŠ¡æŸ¥ä¸åˆ°ç›¸å…³ä¿¡æ¯ã€‚ä½†æˆ‘ä¸æ­»å¿ƒï¼Œè§‰å¾—è‚¯å®šæœ‰åŠæ³•èƒ½ç»“åˆç½‘ç»œçˆ¬è™«æ¥å®ç°è¿™å¥—æµç¨‹ã€‚æˆ‘å¼€å§‹æŸ¥æ‰¾æ˜¯å¦æœ‰ä¸­å›½æœ¬åœ°æ•°æ®åº“æˆ–ç½‘ç«™èƒ½æä¾›ä¹¦ç±çš„å…ƒæ•°æ®ã€‚\n",
    "\n",
    "ğŸ”ºæˆ‘æ‰¾åˆ°äº†ä¸€ä¸ªä¸­æ–‡å›¾ä¹¦åœ¨çº¿é›¶å”®ç½‘ç«™ï¼š[bookschina.com](https://bookschina.com/)ï¼Œç”¨å¤§çº¦70ä¸ªISBNè¿›è¡Œäº†æµ‹è¯•ã€‚æˆ‘ä½¿ç”¨äº†Pythonä¸­çš„BeautifulSoupåº“è¿›è¡Œçˆ¬å–ï¼ŒæˆåŠŸè·å–äº†ä¹¦ç±ä¿¡æ¯ã€‚ä¸‹é¢æ˜¯æˆ‘å¦‚ä½•ä½¿ç”¨Pythonå’ŒBeautifulSoupçˆ¬å–è¯¥ç½‘ç«™ä¿¡æ¯çš„ç®€è¦è®°å½•ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567c58c",
   "metadata": {},
   "source": [
    "### **â†’ Scraping an Online Retailer Page for Book Info with ISBN**  \n",
    "\n",
    "**â†’ 1. Scan book ISBNs with an app on your phone and create a CSV**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;I used an app called [QRbot](https://qrbot.net/locale/en/) which allows you to scan ISBN codes using your phone camera and export a CSV file from your scan history.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;This results in a CSV with a list of ISBNs under a column named `CODECONTENT`, which I saved in the `data/` folder.  \n",
    "\n",
    "**â†’ 2. Set up**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;To set up Python, I recommend following the Programming Historian guide:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Introduction and Installation â€“ Programming Historian](https://programminghistorian.org/en/lessons/introduction-and-installation)  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**Python Library Requirements:**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- `pandas`  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- `beautifulsoup4`  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- `requests`  \n",
    "\n",
    "**â†’ 3. Get a list of product detail page URLs**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The scraping method requires identifying a URL pattern that leads to the productâ€™s detail page, so we can extract relevant data from its HTML.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For [https://bookschina.com/](https://bookschina.com/), the website uses a structured query format for ISBNs that looks like this:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bookschina.com/book_find2/?stp={{ISBN}}&sCate=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb81731",
   "metadata": {},
   "source": [
    "  I can use this URL to find search result pages for each of our ISBNs in the scanned books list **(scanResults.csv)**.\n",
    "  Additionally, since this is just the search result page, I needed to extract the link to the product detail page so that I can get more information on the book.\n",
    "\n",
    "  â†’ <img src=\"../images/0202.png\" alt=\"bookschina.com Search Result with Highlight\" width=\"400px\">\n",
    "  \n",
    "  to get the link, I needed to find the element for it in html\n",
    "\n",
    "  â†’ <img src=\"../images/0204.png\" alt=\"bookschina.com Search Result with Highlight\" width=\"400px\">\n",
    "\n",
    "  In this case, when I append the href to the detail page url format, I get: 'https://www.bookschina.com/{href}.htm', which then led me to the product detail page.\n",
    "\n",
    "\n",
    "  â†’ <img src=\"../images/0203.png\" alt=\"bookschina.com Search Result with Highlight\" width=\"400px\">\n",
    "\n",
    "  Using a 'for' loop, I found detail page URLs for all the ISBNs that returned a result on the website with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e27e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Load scanned ISBNs from the csv in your data folder named scannedResults.csv. \n",
    "# Make sure that the file name is correct\n",
    "df = pd.read_csv('../data/scannedResults.csv')\n",
    "\n",
    "# Create a list to store the extracted hrefs\n",
    "extracted_links = []\n",
    "\n",
    "# iterate over all ISBNs in the [CODECONTENT] column in the csv. \n",
    "# Make sure that the column containing ISBNs in your csv is named correctly.\n",
    "for isbn in df['CODECONTENT']:\n",
    "    url = f'https://bookschina.com/book_find2/?stp={isbn}&sCate=2'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            h2_tag = soup.find('h2', class_='name')\n",
    "            if h2_tag and h2_tag.a:\n",
    "                link = h2_tag.a.get('href')\n",
    "                extracted_links.append(link)\n",
    "            else:\n",
    "                extracted_links.append(None)\n",
    "\n",
    "\n",
    "# Add extracted links to the list\n",
    "df['bookschina_href'] = extracted_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526d421",
   "metadata": {},
   "source": [
    "**â†’ 3. Get info from the product detail page**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Analyzing the product detail page, I found the elements for each field I wanted to extract.\n",
    "\n",
    "  â†’ <img src=\"../images/0205.png\" alt=\"bookschina.com Search Result with Highlight\" width=\"400px\">\n",
    "  â†’ <img src=\"../images/0206.png\" alt=\"bookschina.com Search Result with Highlight\" width=\"400px\">\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;With the following code, I iterated over all the product page URLs and from each page extracted the title, author, publisher, publishing date, page numbers, description, and author introduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#headers for requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "\n",
    "# List to store extracted book info \n",
    "book_data = []\n",
    "\n",
    "for href in df['bookschina_href']:\n",
    "    if pd.isna(href):\n",
    "        book_data.append({\n",
    "            'title': None,\n",
    "            'author': None,\n",
    "            'publisher': None,\n",
    "            'publish_date': None,\n",
    "            'pages': None,\n",
    "            'description': None,\n",
    "            'author_intro': None\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    url = f'https://www.bookschina.com{href}.htm'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # title\n",
    "        title_tag = soup.find(\"title\")\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        # author name\n",
    "        author_tag = soup.find(\"div\", class_=\"author\")\n",
    "        author = author_tag.find(\"a\").text.strip() if author_tag else None\n",
    "\n",
    "        # Publisher and Publishing Date\n",
    "        publisher_tag = soup.find(\"div\", class_=\"publisher\")\n",
    "        publisher = publisher_tag.find_all(\"a\")[0].text.strip() if publisher_tag else None\n",
    "        publish_date = publisher_tag.find(\"i\").text.strip() if publisher_tag and publisher_tag.find(\"i\") else None\n",
    "\n",
    "        # page number\n",
    "        other_info = soup.find(\"div\", class_=\"otherInfor\")\n",
    "        pages = None\n",
    "        if other_info:\n",
    "            span_tags = other_info.find_all(\"span\")\n",
    "            for idx, span in enumerate(span_tags):\n",
    "                if \"é¡µæ•°\" in span.text:\n",
    "                    pages_tag = other_info.find_all(\"i\")[0]\n",
    "                    pages = pages_tag.text.strip() if pages_tag else None\n",
    "\n",
    "        # description\n",
    "        desc_tag = soup.find(\"div\", class_=\"brief\")\n",
    "        description = desc_tag.find(\"p\").text.strip() if desc_tag else None\n",
    "\n",
    "        # author intro\n",
    "        intro_tag = soup.find(\"div\", class_=\"excerpt\")\n",
    "        author_intro = intro_tag.find(\"p\").text.strip() if intro_tag else None\n",
    "\n",
    "        book_data.append({\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'publisher': publisher,\n",
    "            'publish_date': publish_date,\n",
    "            'pages': pages,\n",
    "            'description': description,\n",
    "            'author_intro': author_intro\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        book_data.append({\n",
    "            'title': None,\n",
    "            'author': None,\n",
    "            'publisher': None,\n",
    "            'publish_date': None,\n",
    "            'pages': None,\n",
    "            'description': None,\n",
    "            'author_intro': None\n",
    "        })\n",
    "\n",
    "\n",
    "# convert to data frame\n",
    "book_info_df = pd.DataFrame(book_data)\n",
    "\n",
    "# Save to CSV\n",
    "book_info_df.to_csv('../data/bookschinaResults.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43968ddb",
   "metadata": {},
   "source": [
    "### **â†’ ä½¿ç”¨ISBNçˆ¬å–å›¾ä¹¦åœ¨çº¿é›¶å”®é¡µé¢ä¿¡æ¯**  \n",
    "\n",
    "**â†’ 1. ä½¿ç”¨æ‰‹æœºAppæ‰«æä¹¦ç±ISBNå¹¶ç”ŸæˆCSVæ–‡ä»¶**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;æˆ‘ä½¿ç”¨äº†ä¸€æ¬¾åä¸º [QRbot](https://qrbot.net/locale/en/) çš„Appï¼Œå®ƒå¯ä»¥ç”¨æ‰‹æœºæ‘„åƒå¤´æ‰«æISBNç ï¼Œå¹¶å°†æ‰«æè®°å½•å¯¼å‡ºä¸ºCSVæ–‡ä»¶ã€‚  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;ç”Ÿæˆçš„CSVæ–‡ä»¶ä¸­åŒ…å«ä¸€åˆ—åä¸º `CODECONTENT` çš„ISBNåˆ—è¡¨ï¼Œæˆ‘å°†å…¶ä¿å­˜åœ¨ `data/` æ–‡ä»¶å¤¹ä¸­ã€‚  \n",
    "\n",
    "**â†’ 2. è®¾ç½®ç¯å¢ƒ**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;è¦è®¾ç½®Pythonç¯å¢ƒï¼Œæˆ‘æ¨èå‚è€ƒ Programming Historian æä¾›çš„æŒ‡å—ï¼š  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Pythonç®€ä»‹ä¸å®‰è£… â€“ Programming Historian](https://programminghistorian.org/en/lessons/introduction-and-installation)  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**æ‰€éœ€çš„Pythonåº“ï¼š**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- `pandas`  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- `beautifulsoup4`  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- `requests`  \n",
    "\n",
    "**â†’ 3. è·å–äº§å“è¯¦æƒ…é¡µçš„URLåˆ—è¡¨**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;çˆ¬å–æ–¹æ³•çš„ç¬¬ä¸€æ­¥æ˜¯è¯†åˆ«å‡ºæŒ‡å‘å›¾ä¹¦è¯¦æƒ…é¡µçš„URLæ¨¡å¼ï¼Œä»¥ä¾¿æˆ‘ä»¬ä»é¡µé¢HTMLä¸­æå–ç›¸å…³æ•°æ®ã€‚  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;å¯¹äº [https://bookschina.com/](https://bookschina.com/)ï¼Œè¯¥ç½‘ç«™é‡‡ç”¨ç»“æ„åŒ–çš„ISBNæŸ¥è¯¢URLæ ¼å¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bookschina.com/book_find2/?stp={{ISBN}}&sCate=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6596f",
   "metadata": {},
   "source": [
    "æˆ‘å¯ä»¥ä½¿ç”¨è¿™ä¸ªURLæ ¼å¼ï¼Œåœ¨æˆ‘ä»¬çš„ISBNæ‰«æç»“æœåˆ—è¡¨ï¼ˆ**scanResults.csv**ï¼‰ä¸­ä¸ºæ¯ä¸ªISBNç”Ÿæˆå¯¹åº”çš„æœç´¢ç»“æœé¡µé¢ã€‚  \n",
    "æ­¤å¤–ï¼Œç”±äºè¿™ä¸ªé¡µé¢åªæ˜¯æœç´¢ç»“æœé¡µï¼Œæˆ‘è¿˜éœ€è¦æå–å…¶ä¸­çš„å•†å“è¯¦æƒ…é¡µé“¾æ¥ï¼Œä»¥ä¾¿è·å–è¯¥ä¹¦çš„æ›´å¤šä¿¡æ¯ã€‚\n",
    "\n",
    "â†’ <img src=\"../images/0202.png\" alt=\"bookschina.comæœç´¢ç»“æœé¡µé¢æˆªå›¾\" width=\"400px\">\n",
    "\n",
    "ä¸ºäº†è·å–é“¾æ¥ï¼Œæˆ‘éœ€è¦åœ¨HTMLä¸­æ‰¾åˆ°å¯¹åº”çš„å…ƒç´ ã€‚\n",
    "\n",
    "â†’ <img src=\"../images/0204.png\" alt=\"bookschina.comæœç´¢ç»“æœé“¾æ¥ç»“æ„æˆªå›¾\" width=\"400px\">\n",
    "\n",
    "åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå½“æˆ‘å°† `href` æ·»åŠ åˆ°è¯¦æƒ…é¡µçš„URLæ ¼å¼ä¸­ï¼Œå³å¾—åˆ°ï¼š`https://www.bookschina.com/{href}.htm`ï¼Œä»è€Œè¿›å…¥è¯¥å›¾ä¹¦çš„å•†å“è¯¦æƒ…é¡µã€‚\n",
    "\n",
    "â†’ <img src=\"../images/0203.png\" alt=\"bookschina.comå•†å“è¯¦æƒ…é¡µæˆªå›¾\" width=\"400px\">\n",
    "\n",
    "æˆ‘ä½¿ç”¨ä¸€ä¸ª `for` å¾ªç¯ï¼Œä¸ºæ‰€æœ‰åœ¨ç½‘ç«™ä¸Šæœ‰ç»“æœçš„ISBNæ„å»ºäº†å¯¹åº”çš„å•†å“è¯¦æƒ…é¡µURLï¼Œä»¥ä¸‹æ˜¯ç›¸å…³ä»£ç ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817fbedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Load scanned ISBNs from the csv in your data folder named scannedResults.csv. \n",
    "# Make sure that the file name is correct\n",
    "df = pd.read_csv('../data/scannedResults.csv')\n",
    "\n",
    "# Create a list to store the extracted hrefs\n",
    "extracted_links = []\n",
    "\n",
    "# iterate over all ISBNs in the [CODECONTENT] column in the csv. \n",
    "# Make sure that the column containing ISBNs in your csv is named correctly.\n",
    "for isbn in df['CODECONTENT']:\n",
    "    url = f'https://bookschina.com/book_find2/?stp={isbn}&sCate=2'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            h2_tag = soup.find('h2', class_='name')\n",
    "            if h2_tag and h2_tag.a:\n",
    "                link = h2_tag.a.get('href')\n",
    "                extracted_links.append(link)\n",
    "            else:\n",
    "                extracted_links.append(None)\n",
    "\n",
    "\n",
    "# Add extracted links to the list\n",
    "df['bookschina_href'] = extracted_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d3d1d",
   "metadata": {},
   "source": [
    "**â†’ 3. ä»å•†å“è¯¦æƒ…é¡µä¸­æå–ä¿¡æ¯**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;åœ¨åˆ†æå•†å“è¯¦æƒ…é¡µæ—¶ï¼Œæˆ‘æ‰¾åˆ°äº†æƒ³è¦æå–çš„å„ä¸ªå­—æ®µå¯¹åº”çš„HTMLå…ƒç´ ã€‚\n",
    "\n",
    "â†’ <img src=\"../images/0205.png\" alt=\"bookschina.comå•†å“è¯¦æƒ…é¡µå­—æ®µç»“æ„æˆªå›¾1\" width=\"400px\">  \n",
    "â†’ <img src=\"../images/0206.png\" alt=\"bookschina.comå•†å“è¯¦æƒ…é¡µå­—æ®µç»“æ„æˆªå›¾2\" width=\"400px\">\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼Œæˆ‘å¯¹æ‰€æœ‰å•†å“è¯¦æƒ…é¡µçš„URLè¿›è¡Œéå†ï¼Œå¹¶ä»æ¯ä¸€é¡µä¸­æå–äº†æ ‡é¢˜ã€ä½œè€…ã€å‡ºç‰ˆç¤¾ã€å‡ºç‰ˆæ—¶é—´ã€é¡µæ•°ã€å†…å®¹ç®€ä»‹å’Œä½œè€…ç®€ä»‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#headers for requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "\n",
    "# List to store extracted book info \n",
    "book_data = []\n",
    "\n",
    "for href in df['bookschina_href']:\n",
    "    if pd.isna(href):\n",
    "        book_data.append({\n",
    "            'title': None,\n",
    "            'author': None,\n",
    "            'publisher': None,\n",
    "            'publish_date': None,\n",
    "            'pages': None,\n",
    "            'description': None,\n",
    "            'author_intro': None\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    url = f'https://www.bookschina.com{href}.htm'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # title\n",
    "        title_tag = soup.find(\"title\")\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        # author name\n",
    "        author_tag = soup.find(\"div\", class_=\"author\")\n",
    "        author = author_tag.find(\"a\").text.strip() if author_tag else None\n",
    "\n",
    "        # Publisher and Publishing Date\n",
    "        publisher_tag = soup.find(\"div\", class_=\"publisher\")\n",
    "        publisher = publisher_tag.find_all(\"a\")[0].text.strip() if publisher_tag else None\n",
    "        publish_date = publisher_tag.find(\"i\").text.strip() if publisher_tag and publisher_tag.find(\"i\") else None\n",
    "\n",
    "        # page number\n",
    "        other_info = soup.find(\"div\", class_=\"otherInfor\")\n",
    "        pages = None\n",
    "        if other_info:\n",
    "            span_tags = other_info.find_all(\"span\")\n",
    "            for idx, span in enumerate(span_tags):\n",
    "                if \"é¡µæ•°\" in span.text:\n",
    "                    pages_tag = other_info.find_all(\"i\")[0]\n",
    "                    pages = pages_tag.text.strip() if pages_tag else None\n",
    "\n",
    "        # description\n",
    "        desc_tag = soup.find(\"div\", class_=\"brief\")\n",
    "        description = desc_tag.find(\"p\").text.strip() if desc_tag else None\n",
    "\n",
    "        # author intro\n",
    "        intro_tag = soup.find(\"div\", class_=\"excerpt\")\n",
    "        author_intro = intro_tag.find(\"p\").text.strip() if intro_tag else None\n",
    "\n",
    "        book_data.append({\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'publisher': publisher,\n",
    "            'publish_date': publish_date,\n",
    "            'pages': pages,\n",
    "            'description': description,\n",
    "            'author_intro': author_intro\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        book_data.append({\n",
    "            'title': None,\n",
    "            'author': None,\n",
    "            'publisher': None,\n",
    "            'publish_date': None,\n",
    "            'pages': None,\n",
    "            'description': None,\n",
    "            'author_intro': None\n",
    "        })\n",
    "\n",
    "\n",
    "# convert to data frame\n",
    "book_info_df = pd.DataFrame(book_data)\n",
    "\n",
    "# Save to CSV\n",
    "book_info_df.to_csv('../data/bookInfoBooksChina.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
