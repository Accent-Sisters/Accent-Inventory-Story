{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d638ea",
   "metadata": {},
   "source": [
    "## ‚Üí Chapter Six \n",
    "\n",
    "**Deciphering the broken pinyin, Completing the Epic Merge**  \n",
    "\n",
    "üî∫One of the challanges in data cleaning was that, global databases like ISBNdb and isbnsearch.org often returned Chinese book metadata in **pinyin**(the phonetic romanization of Chinese) and not in readable Chinese characters. To fix this, I used a Python library called **pinyin2hanzi** which converts pinyin text into the most likely Chinese characters. \n",
    "\n",
    "üî∫With the code below, I was able to transform some of the titles in pinyin into Chinese and clean up the title values from dangdang.com best as I could.\n",
    "\n",
    "üî∫Since there are also English titles mixed in, I specified the confidence score to be > 0.01 to select only the accurate results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43a62cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Pinyin2Hanzi import DefaultDagParams, dag\n",
    "\n",
    "# initialize pinyin converter\n",
    "dagparams = DefaultDagParams()\n",
    "\n",
    "# helper function to convert pinyin string to hanzi if score > 0.1\n",
    "def convert_pinyin_to_hanzi(pinyin_string):\n",
    "    pinyin_list = pinyin_string.strip().lower().split()\n",
    "    try:\n",
    "        result = dag(dagparams, pinyin_list, path_num=5)\n",
    "        for item in result:\n",
    "            if item.score > 0.01:\n",
    "                return ''.join(item.path)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- Convert isbndbResults.csv ---\n",
    "isbndb_df = pd.read_csv('../data/isbndbResults.csv')\n",
    "isbndb_df['converted_title'] = isbndb_df['title'].astype(str).apply(convert_pinyin_to_hanzi)\n",
    "\n",
    "# --- Convert zoteroExport.csv ---\n",
    "zotero_df = pd.read_csv('../data/zoteroExport.csv')\n",
    "\n",
    "# process title\n",
    "zotero_df['converted_title'] = zotero_df['Title'].astype(str).apply(convert_pinyin_to_hanzi)\n",
    "\n",
    "# clean ISBN column (keep first 17 characters, remove hyphens)\n",
    "zotero_df['ISBN'] = zotero_df['ISBN'].astype(str).str[:17].str.replace('-', '', regex=False)\n",
    "\n",
    "# result DataFrames\n",
    "isbndbConverted = isbndb_df\n",
    "zoteroConverted = zotero_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33491b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV\n",
    "dangdang = pd.read_csv('../data/dangdangResults.csv')\n",
    "\n",
    "# Clean title: remove all full-width parentheses ÔºàÔºâ and „Äê„Äë\n",
    "def clean_title(title):\n",
    "    if pd.isna(title):\n",
    "        return title\n",
    "    title = re.sub(r'Ôºà.*?Ôºâ', '', title)\n",
    "    title = re.sub(r'„Äê.*?„Äë', '', title)\n",
    "    return title.strip()\n",
    "\n",
    "# Apply cleaning\n",
    "dangdang['title'] = dangdang['title'].astype(str).apply(clean_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c990a5d5",
   "metadata": {},
   "source": [
    "üî∫However, a lot of these works' titles are too complicated for the library to convert, so I decided to do a manual edit on the final result to: transform the rest of the pinyin and clean the rest of the dangdang titles. \n",
    "\n",
    "üî∫Then, I created a script to merge the book data collected from the four sources into a single, structured dataset formatted to match the fields required for Shopify product import.\n",
    "- **ISBNdb** \n",
    "- **Zotero** \n",
    "- **Dangdang.com** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e057c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/shopifyTemplate.csv')\n",
    "dangdang = pd.read_csv('../data/dangdangResults.csv')\n",
    "zotero = pd.read_csv('../data/zoteroExport.csv')\n",
    "isbndb = pd.read_csv('../data/isbndbResults.csv')\n",
    "\n",
    "# Clean up ISBNs to match format\n",
    "zotero['ISBN'] = zotero['ISBN'].astype(str).str.replace('-', '').str[:13]\n",
    "isbndb['isbn_searched'] = isbndb['isbn_searched'].astype(str).str[:13]\n",
    "dangdang['isbn'] = dangdang['isbn'].astype(str).str[:13]\n",
    "\n",
    "# Helper function to get first non-null value from priority list\n",
    "def get_priority_value(isbn, zot_col, isbndb_col, dang_col):\n",
    "    if isbn in zotero['ISBN'].values:\n",
    "        return zotero.loc[zotero['ISBN'] == isbn, zot_col].values[0]\n",
    "    elif isbn in isbndb['isbn_searched'].values:\n",
    "        return isbndb.loc[isbndb['isbn_searched'] == isbn, isbndb_col].values[0]\n",
    "    elif isbn in dangdang['isbn'].values:\n",
    "        return dangdang.loc[dangdang['isbn'] == isbn, dang_col].values[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply field population based on hierarchy\n",
    "df['Variant Barcode'] = df['Variant Barcode'].astype(str).str[:13]  # Ensure consistency\n",
    "df['Title'] = df['Variant Barcode'].apply(lambda x: get_priority_value(x, 'Title', 'title', 'title'))\n",
    "df['Image Src'] = df['Variant Barcode'].apply(lambda x: isbndb.loc[isbndb['isbn_searched'] == x, 'image'].values[0]\n",
    "                                              if x in isbndb['isbn_searched'].values else None)\n",
    "\n",
    "# Construct Body (HTML)\n",
    "def build_html_body(isbn):\n",
    "    if isbn in zotero['ISBN'].values:\n",
    "        row = zotero.loc[zotero['ISBN'] == isbn].iloc[0]\n",
    "        year = row.get('Publication Year', '')\n",
    "        author = row.get('Author', '')\n",
    "        publisher = row.get('Publisher', '')\n",
    "        pages = row.get('Num Pages', '')\n",
    "        description = row.get('Abstract Note', '')\n",
    "    elif isbn in isbndb['isbn_searched'].values:\n",
    "        row = isbndb.loc[isbndb['isbn_searched'] == isbn].iloc[0]\n",
    "        year = row.get('date_published', '')\n",
    "        author = row.get('authors', '')\n",
    "        publisher = row.get('publisher', '')\n",
    "        pages = row.get('pages', '')\n",
    "        description = row.get('synopsis', '')\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "    return f\"<p>Year: {year}</p><p>Author: {author}</p><p>Publisher: {publisher}</p><p>Pages: {pages}</p><p>Description: {description}</p>\"\n",
    "\n",
    "df['Body (HTML)'] = df['Variant Barcode'].apply(build_html_body)\n",
    "df.to_csv('finalInventory.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59783c0f",
   "metadata": {},
   "source": [
    "due to restrictions, I cannot provide the final data sheet of the full inventory result. The finalInventory.csv file in the data folder is a sample of the final result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
